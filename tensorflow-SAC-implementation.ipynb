{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, memory_size, obs_dim, action_dim):\n",
    "        self.memory_size = memory_size  # maximal memory size\n",
    "        self.memory_counter = 0 \n",
    "        self.state_memory = np.zeros((self.memory_size, *obs_dim))\n",
    "        self.next_state_memory = np.zeros((self.memory_size, *obs_dim))\n",
    "        self.action_memory = np.zeros((self.memory_size, action_dim))\n",
    "        self.reward_memory = np.zeros(self.memory_size)\n",
    "        self.done_memory = np.zeros(self.memory_size, dtype=np.bool)\n",
    "\n",
    "    # Stores the transition in the memory\n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory_counter += 1\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.done_memory[index] = done\n",
    "    \n",
    "    # Get random batch from the memory\n",
    "    def sample_batch(self, batch_size):\n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "        batch_index = np.random.choice(max_memory, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch_index]\n",
    "        actions = self.action_memory[batch_index]\n",
    "        rewards = self.reward_memory[batch_index]\n",
    "        next_states = self.next_state_memory[batch_index]\n",
    "        dones = self.done_memory[batch_index]\n",
    "\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tf.random.set_seed(42)  \n",
    "\n",
    "class CriticNetwork(Model):\n",
    "    def __init__(self, action_dim, neurons=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.layer1 = Dense(neurons, activation='relu')\n",
    "        self.layer2 = Dense(neurons, activation='relu')\n",
    "        self.q = Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, state, action):\n",
    "        state_action_value = self.layer1(tf.concat([state, action], axis=1))\n",
    "        state_action_value = self.layer2(state_action_value)\n",
    "        q = self.q(state_action_value)\n",
    "        return q\n",
    "\n",
    "class ActorNetwork(Model):\n",
    "    def __init__(self, action_dim, action_limit, neurons=256, init_weight=3e-3):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.noise = 1e-6\n",
    "        self.action_limit = action_limit\n",
    "\n",
    "        self.layer1 = Dense(neurons, activation='relu')\n",
    "        self.layer2 = Dense(neurons, activation='relu')\n",
    "        self.layer3 = Dense(neurons, activation='relu')\n",
    "        self.mean = Dense(self.action_dim, activation='linear', bias_initializer=tf.random_uniform_initializer(-init_weight, init_weight))\n",
    "        self.log_std = Dense(self.action_dim, activation='linear', bias_initializer=tf.random_uniform_initializer(-init_weight, init_weight))\n",
    "    \n",
    "    \n",
    "    def call(self, state):\n",
    "        x = self.layer1(state)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        mean = self.mean(x) \n",
    "        log_std = self.log_std(x)\n",
    "        log_std = tf.clip_by_value(log_std, -20, 2)\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.call(state)\n",
    "        std = tf.math.exp(log_std) \n",
    "\n",
    "        normal = tfd.Normal(0, 1)\n",
    "        z = normal.sample(tf.shape(mean))\n",
    "        action_0 = tf.math.tanh(mean + std * z) # Tanh squashes the action to be between -1 and 1\n",
    "        action = self.action_limit * action_0 # Normalize actions with enviroment action range (not necessary for Walker)\n",
    "        log_pi = tfd.Normal(mean, std).log_prob(mean + std * z) # log probability of normal distribution\n",
    "        log_pi -= tf.math.log(1.0 - action_0 ** 2 + self.noise) - np.log(self.action_limit) # Add correction for Tanh\n",
    "        log_pi = tf.reduce_sum(log_pi, axis=1)[:, np.newaxis] # Expand dim because reduce_sum reduces dim\n",
    "\n",
    "        return action, log_pi\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = tf.convert_to_tensor([state])\n",
    "        mean, log_std = self.call(state)\n",
    "        std = tf.math.exp(log_std)\n",
    "\n",
    "        normal = tfd.Normal(0, 1)\n",
    "        z = normal.sample(tf.shape(mean))\n",
    "        action = self.action_limit * tf.math.tanh(mean + std * z)\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, obs_dim, action_dim, gamma,\n",
    "                tau, lr, env=None, memory_size=1000000, \n",
    "                batch_size=256, dir='walker 4'):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_limit = env.action_space.high\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.tau = tau # Interpolation factor for updating target networks\n",
    "        self.lr = lr # Learning rate for all networks\n",
    "        self.memory = Memory(memory_size, obs_dim, action_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.dir = dir\n",
    "        self.log_alpha = tf.Variable(0, dtype=np.float32, name='log_alpha')\n",
    "        # self.alpha = tf.math.exp(self.log_alpha)\n",
    "        self.alpha = 0.2\n",
    "        self.target_entropy = -1. * action_dim # -dim(actions)\n",
    "\n",
    "\n",
    "        # Policy Network (Actor)\n",
    "        self.actor = ActorNetwork(action_dim=action_dim, action_limit = self.action_limit)\n",
    "\n",
    "        # 2 Critic Networks and 2 Target Networks\n",
    "        self.critic1 = CriticNetwork(action_dim=action_dim)\n",
    "        self.target1 = CriticNetwork(action_dim=action_dim)\n",
    "        self.critic2 = CriticNetwork(action_dim=action_dim)\n",
    "        self.target2 = CriticNetwork(action_dim=action_dim)\n",
    "\n",
    "        # Initialize target network weights \n",
    "        # tau = 1 -> hard update\n",
    "        self.target1 = self.soft_update(self.critic1, self.target1, 1)\n",
    "        self.target2 = self.soft_update(self.critic2, self.target2, 1)\n",
    "\n",
    "\n",
    "        self.critic1_opt = tf.optimizers.legacy.Adam(self.lr)\n",
    "        self.critic2_opt = tf.optimizers.legacy.Adam(self.lr)\n",
    "        self.actor_opt = tf.optimizers.legacy.Adam(self.lr)\n",
    "        self.alpha_opt = tf.optimizers.legacy.Adam(self.lr) # optimizer for automatic entropy regularization\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # Update Target Network according to tau*target_weights + (1-tau)*critic_weights\n",
    "    # Hard copy if tau=1\n",
    "    def soft_update(self, critic, target, tau):\n",
    "        for target_weight, critic_weight in zip(target.trainable_weights, critic.trainable_weights):\n",
    "            target_weight.assign(target_weight * (1.0 - tau) + critic_weight * tau)\n",
    "        return target\n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "        state, action, reward, next_state, done = self.memory.sample_batch(self.batch_size)\n",
    "\n",
    "        reward = reward[:, np.newaxis]  # Expand dim\n",
    "        done = done[:, np.newaxis]\n",
    "\n",
    "        reward = reward - np.mean(reward, axis=0) / (np.std(reward, axis=0) + 1e-6)  # Normalize reward with batch\n",
    "        \n",
    "\n",
    "        next_action, log_pi = self.actor.sample(next_state)\n",
    "        target_value1 = self.target1(next_state, next_action)\n",
    "        target_value2 = self.target2(next_state, next_action)\n",
    "        target_value = tf.math.minimum(target_value1, target_value2) - self.alpha * log_pi # Entropy-regularized Bellman\n",
    "\n",
    "        target_q = reward + self.gamma * (1-done) * target_value \n",
    "\n",
    "        # Training based on gradients\n",
    "        self.train_critic1(target_q, state, action)\n",
    "        self.train_critic2(target_q, state, action)\n",
    "        log_pi = self.train_actor(state)\n",
    "        \n",
    "        # Auto regularization of entropy coefficient\n",
    "        # self.auto_alpha(log_pi)\n",
    "\n",
    "        # Update Target Networks\n",
    "        self.target1 = self.soft_update(self.critic1, self.target1, self.tau)\n",
    "        self.target2 = self.soft_update(self.critic2, self.target2, self.tau)\n",
    "\n",
    "    def train_critic1(self, target_q, state, action):\n",
    "        with tf.GradientTape() as tape_q1:\n",
    "            q1 = self.critic1(state, action)\n",
    "            self.q1_loss = tf.reduce_mean(tf.losses.mean_squared_error(q1, target_q))\n",
    "        q1_gradient= tape_q1.gradient(self.q1_loss, self.critic1.trainable_weights)\n",
    "        self.critic1_opt.apply_gradients(zip(q1_gradient, self.critic1.trainable_weights))\n",
    "\n",
    "    def train_critic2(self, target_q, state, action):\n",
    "        with tf.GradientTape() as tape_q2:\n",
    "            q2 = self.critic2(state, action)\n",
    "            self.q2_loss = tf.reduce_mean(tf.losses.mean_squared_error(q2, target_q))\n",
    "        q2_gradient = tape_q2.gradient(self.q2_loss, self.critic2.trainable_weights)\n",
    "        self.critic2_opt.apply_gradients(zip(q2_gradient, self.critic2.trainable_weights))\n",
    "    \n",
    "    def train_actor(self, state):\n",
    "        with tf.GradientTape() as tape_actor:\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "            q = tf.math.minimum(self.critic1(state, action), self.critic2(state, action))\n",
    "            self.actor_loss = tf.reduce_mean(self.alpha * log_pi - q)\n",
    "        actor_gradient = tape_actor.gradient(self.actor_loss, self.actor.trainable_weights)\n",
    "        self.actor_opt.apply_gradients(zip(actor_gradient, self.actor.trainable_weights))\n",
    "        return log_pi\n",
    "    \n",
    "    def auto_alpha(self, log_pi):\n",
    "        with tf.GradientTape() as alpha_tape:\n",
    "            alpha_loss = -tf.reduce_mean((self.log_alpha * (log_pi + self.target_entropy)))\n",
    "        alpha_grad = alpha_tape.gradient(alpha_loss, [self.log_alpha])\n",
    "        self.alpha_opt.apply_gradients(zip(alpha_grad, [self.log_alpha]))\n",
    "        self.alpha = tf.math.exp(self.log_alpha)\n",
    " \n",
    "    def add_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add_memory(state, action, reward, next_state, done)\n",
    "    \n",
    "    def save(self):\n",
    "        self.actor.save_weights(self.dir + \"/actor.ckpt\")\n",
    "        self.critic1.save_weights(self.dir + \"/critic1.ckpt\")\n",
    "        self.critic2.save_weights(self.dir + \"/critic2.ckpt\")\n",
    "        self.target1.save_weights(self.dir + \"/target1.ckpt\")\n",
    "        self.target2.save_weights(self.dir + \"/target2.ckpt\")\n",
    "        print(\"Models saved\")\n",
    "    \n",
    "    def load(self):\n",
    "        self.actor.load_weights(self.dir+\"/actor.ckpt\")\n",
    "        self.critic1.load_weights(self.dir+\"/critic1.ckpt\")\n",
    "        self.critic2.load_weights(self.dir+\"/critic2.ckpt\")\n",
    "        self.target1.load_weights(self.dir+\"/target1.ckpt\")\n",
    "        self.target2.load_weights(self.dir+\"/target2.ckpt\")\n",
    "        print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michaelfuglo/Projects/BipedalWalker2D'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# print (os.getcwd())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/27kk04gn68z8t3bc0npl90x00000gn/T/ipykernel_18468/540153097.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.done_memory = np.zeros(self.memory_size, dtype=np.bool)\n",
      "Training:   0%|          | 0/2000 [00:00<?, ? episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/2000 [00:44<12:27:50, 22.46s/ episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Models saved\n",
      "Score -77.3 Average Score -88.3 Steps 1600.0 Total Steps 1680.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/2000 [01:34<18:47:57, 33.89s/ episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Score -103.8 Average Score -93.5 Steps 1600.0 Total Steps 3280.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/2000 [01:43<19:12:31, 34.63s/ episode]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/18/27kk04gn68z8t3bc0npl90x00000gn/T/ipykernel_18468/3694909235.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sample random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sample action from policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/18/27kk04gn68z8t3bc0npl90x00000gn/T/ipykernel_18468/3967740787.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Training based on gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_critic1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_critic2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mlog_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Auto regularization of entropy coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/18/27kk04gn68z8t3bc0npl90x00000gn/T/ipykernel_18468/3967740787.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target_q, state, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_critic2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape_q2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq2_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mq2_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape_q2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic2_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                     )\n\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/18/27kk04gn68z8t3bc0npl90x00000gn/T/ipykernel_18468/175408101.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mstate_action_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstate_action_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                     )\n\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/keras/layers/core/dense.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   3550\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3551\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3552\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3554\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/BipedalWalker2D/.venv/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    851\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       return bias_add_eager_fallback(\n\u001b[1;32m    858\u001b[0m           value, bias, data_format=data_format, name=name, ctx=_ctx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 0.02\n",
    "learning_rate = 0.0003\n",
    "memory_size = 1000000\n",
    "batch_size = 128\n",
    "start_steps = 2000\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    env.seed(42)\n",
    "    agent = Agent(obs_dim=env.observation_space.shape,\n",
    "            action_dim=env.action_space.shape[0], gamma=gamma,\n",
    "            tau=tau, lr=learning_rate, env=env, memory_size=memory_size, \n",
    "            batch_size=batch_size, dir='walker 6')\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    max_steps = env._max_episode_steps\n",
    "    total_steps = 0\n",
    "    scores = []\n",
    "    average_actor_loss = []\n",
    "    average_q1_loss = []\n",
    "    average_q2_loss = []\n",
    "    steps = []\n",
    "    alphas = []\n",
    "    progress = tqdm(range(2000), desc='Training', unit=' episode')\n",
    "    for i in progress:\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        episode_steps = 0\n",
    "        actor_loss = []\n",
    "        q1_loss = []\n",
    "        q2_loss = []\n",
    "        for step in range(max_steps):\n",
    "\n",
    "            if start_steps > total_steps:\n",
    "                action = env.action_space.sample()  # Sample random action\n",
    "            else:\n",
    "                action = agent.actor.choose_action(observation)  # Sample action from policy\n",
    "            \n",
    "\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            next_observation = next_observation.astype(np.float32)\n",
    "            score += reward\n",
    "            episode_steps += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            done = 1 if done is True else 0\n",
    "\n",
    "            agent.add_memory(observation, action, reward, next_observation, done)\n",
    "            observation = next_observation\n",
    "            if total_steps > batch_size:\n",
    "                agent.learn()\n",
    "                actor_loss.append(agent.actor_loss)\n",
    "                q1_loss.append(agent.q1_loss)\n",
    "                q2_loss.append(agent.q2_loss)\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(score)\n",
    "        steps.append(episode_steps)\n",
    "        alphas.append(agent.alpha)\n",
    "        if total_steps > batch_size:\n",
    "            actor_loss = np.mean(actor_loss)\n",
    "            q1_loss = np.mean(q1_loss)\n",
    "            q2_loss = np.mean(q2_loss)\n",
    "            average_actor_loss.append(actor_loss)\n",
    "            average_q1_loss.append(q1_loss)\n",
    "            average_q2_loss.append(q2_loss)\n",
    "\n",
    "        average_score = np.mean(scores[-100:])\n",
    "        print('\\n')\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            agent.save()\n",
    "        if i % 20:\n",
    "            np.save(\"actor_loss.npy\", np.array(average_actor_loss))\n",
    "            np.save(\"critic1_loss.npy\", np.array(average_q1_loss))\n",
    "            np.save(\"critic2_loss.npy\", np.array(average_q2_loss))\n",
    "            np.save(\"reward.npy\", np.array(scores))\n",
    "            np.save(\"steps.npy\", np.array(steps))\n",
    "            np.save(\"alpha.npy\", np.array(alphas))\n",
    "        if total_steps > batch_size:\n",
    "            #print('Actor Loss:  %.4f ' % actor_loss, 'Q1 Loss: %.4f' % q1_loss,'Q2 Loss: %.4f' % q2_loss, 'Alpha: %.4f' % agent.alpha)\n",
    "            #print('Average Actor Loss: %.4f' % np.mean(average_actor_loss[-100:]), 'Average Q1 Loss: %.4f'  % np.mean(average_q1_loss[-100:]),\n",
    "            #     'Average Q2 Loss: %.4f'  % np.mean(average_q2_loss[-100:]))\n",
    "            print('Score %.1f' % score, 'Average Score %.1f' % average_score, 'Steps %.1f' % episode_steps, 'Total Steps %.1f' % total_steps)\n",
    "        if average_score > 300:\n",
    "            break\n",
    "    np.save(\"actor_loss.npy\", np.array(average_actor_loss))\n",
    "    np.save(\"critic1_loss.npy\", np.array(average_q1_loss))\n",
    "    np.save(\"critic2_loss.npy\", np.array(average_q2_loss))\n",
    "    np.save(\"reward.npy\", np.array(scores))\n",
    "    np.save(\"steps.npy\", np.array(steps))\n",
    "    np.save(\"alpha.npy\", np.array(alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m'\u001b[39m\u001b[39mBipedalWalker-v3\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m env\u001b[39m.\u001b[39mseed(\u001b[39m42\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m agent \u001b[39m=\u001b[39m Agent(obs_dim\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape,\n\u001b[1;32m     21\u001b[0m         action_dim\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], gamma\u001b[39m=\u001b[39mgamma,\n\u001b[1;32m     22\u001b[0m         tau\u001b[39m=\u001b[39mtau, lr\u001b[39m=\u001b[39mlearning_rate, env\u001b[39m=\u001b[39menv, memory_size\u001b[39m=\u001b[39mmemory_size,\n\u001b[1;32m     23\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size, \u001b[39mdir\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwalker 6\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m agent\u001b[39m.\u001b[39mload()\n\u001b[1;32m     26\u001b[0m best_score \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreward_range[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Agent' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "os.chdir(\"/Users/michaelfuglo/Projects/BipedalWalker2D\") # Change to your Folder\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "learning_rate = 0.0003\n",
    "memory_size = 1000000\n",
    "batch_size = 256\n",
    "test_episode = 5\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    env.seed(42)\n",
    "    agent = Agent(obs_dim=env.observation_space.shape,\n",
    "            action_dim=env.action_space.shape[0], gamma=gamma,\n",
    "            tau=tau, lr=learning_rate, env=env, memory_size=memory_size,\n",
    "            batch_size=batch_size, dir='walker 6')\n",
    "\n",
    "    agent.load()\n",
    "    best_score = env.reward_range[0]\n",
    "    max_steps = 1600\n",
    "    t0 = time.time()\n",
    "    for episode in range(test_episode):\n",
    "        total_steps = 0\n",
    "        score = 0\n",
    "        episode_steps = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        for step in range(max_steps):\n",
    "            env.render()\n",
    "\n",
    "            action = agent.actor.choose_action(observation)\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            next_observation = next_observation.astype(np.float32)\n",
    "            score += reward\n",
    "            episode_steps += 1\n",
    "            total_steps += 1\n",
    "\n",
    "            observation = next_observation\n",
    "            if done:\n",
    "                break\n",
    "        print(\n",
    "            'Testing  | Episode: {}/{}  | Episode Reward: {:.4f} |  | Running Time: {:.4f}'.format(\n",
    "                episode + 1, test_episode, score,\n",
    "                time.time() - t0\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
